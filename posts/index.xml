<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on MLBlog</title>
    <link>http://repun.github.io/posts/</link>
    <description>Recent content in Posts on MLBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 21 Feb 2024 09:52:08 +0900</lastBuildDate><atom:link href="http://repun.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Transformer에 사용되는 Positional Embedding</title>
      <link>http://repun.github.io/posts/positional_embedding/</link>
      <pubDate>Wed, 21 Feb 2024 09:52:08 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/positional_embedding/</guid>
      <description>이번에는 Transformer 구조에서 어떻게 인풋 시퀀스의 순서 정보를 받아들이는지에 대한 이야기를 해보도록 하겠습니다. Transformer에 사용되는 Self-Attention은 구조적으로 인풋 시퀀스의 순서를 구분하지 못합니다. 그래서 순서를 구분하기 위한 목적으로 시퀀스 각 벡터의 위치 정보를 넣은 임베딩 벡터를 사용하게 되는데 이걸 Positional Embedding(PE)이라고 부릅니다. 이 글에서는 이런 PE 기법들의 목적과 논리에 대해 정리해보도록 할게요. PE가 모델 내에서 무슨 역할을 어떻게 하는지, 어떤 논리와 이유를 가지고 디자인 되었는지를 이해하는데 도움 된다면 좋겠습니다.
Transformer에 사용되는 Self-Attention에 대한 이야기부터 시작해보도록 합시다.</description>
    </item>
    
    <item>
      <title>CTC 디코딩을 위한 Prefix Beam Search</title>
      <link>http://repun.github.io/posts/prefix_beam_search/</link>
      <pubDate>Sun, 16 Oct 2022 20:47:24 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/prefix_beam_search/</guid>
      <description>이번 글은 Prefix Beam Search 알고리즘에 대해 정리하는 글입니다. Prefix Beam Search는 CTC(Connectionist Temporal Classification)를 이용해 학습된 모델의 디코딩에 사용됩니다. 평범한 Beam Search 알고리즘에서 CTC 디코딩에 맞게 살짝 변형시킨 알고리즘이 바로 Prefix Beam Search에요. 그러니 당연히 Prefix Beam Search에 대해 알기 위해서는 CTC와 Beam Search, 두 알고리즘에 대해 알고 있어야 합니다. 그래야 Prefix Beam Search 알고리즘이 왜 필요하고 어떻게 동작하는지 이해할 수 있습니다.
이 글은 Prefix Beam Search를 이해하는 데에 초점을 맞출 예정이에요.</description>
    </item>
    
    <item>
      <title>AMP(Automatic Mixed Precision)를 이용한 신경망 학습</title>
      <link>http://repun.github.io/posts/amp/</link>
      <pubDate>Mon, 08 Aug 2022 17:16:35 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/amp/</guid>
      <description>이번 글은 Automatic Mixed-Precision(AMP)를 이용해 신경망을 학습하는 원리를 간단하게 정리합니다. Mixed-Precision이라는건 말 그대로 Precision을 섞었다는 의미입니다. 보통 PyTorch나 TensorFlow 등의 딥러닝 프레임워크를 이용해 신경망 모델을 학습시킬 경우, 따로 설정을 하지 않는 이상 32-bit 크기의 소수 자료형을 이용해 계산합니다. 이 32-bit 크기의 소수 자료형을 Single Precision 혹은 FP32라고 부르지요. 하지만 32-bit의 절반, 16-bit만을 이용해 소수를 표현할 수도 있습니다. 이 자료형을 Half Precision 혹은 FP16 이라고 부릅니다. FP16은 FP32에 비해 메모리도 적게 먹고 빠르게 계산할 수 있지만, 표현할 수 있는 소수의 범위가 작습니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 Attention Mechanism</title>
      <link>http://repun.github.io/posts/attention_mechanism/</link>
      <pubDate>Tue, 24 May 2022 17:17:44 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/attention_mechanism/</guid>
      <description>이번 글에서는 딥러닝에서 사용되는 어텐션(Attention) 메커니즘에 대해 정리하려고 합니다. 우선 어텐션이 어떤 논리로 작동하는지에 대해 간단하게 정리할게요. 기본이 되는 논리를 정확하게 이해하고 있으면 다양하게 제안되어온 수많은 어텐션의 구조와 수식을 이해하는데에 문제가 없습니다. 어텐션의 기본 논리 이후에는 어텐션이 사용된 예시를 몇가지 확인하도록 하겠습니다.
어텐션은 영단어의 뜻 그대로 집중이라는 의미입니다. 수많은 정보들 사이에서 내가 어떤 부분에 집중해야 하는가? 를 신경망으로 나타낸 구조라고 생각할 수 있어요. 신경망으로 집중을 나타낸다니 무슨 소리인지 감이 안올 수 있지만, 어텐션은 우리가 사전이나 데이터베이스에서 원하는 정보를 찾는 과정과 구조가 닮았습니다.</description>
    </item>
    
    <item>
      <title>Kernel K-Means 클러스터링의 원리</title>
      <link>http://repun.github.io/posts/kmeans_clustering/</link>
      <pubDate>Sun, 27 Feb 2022 09:52:08 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/kmeans_clustering/</guid>
      <description>이번 글은 K-means 클러스터링 알고리즘에 대해 정리해보도록 하겠습니다. K-means 클러스터링은 간단하지만 성능이 좋아 자주 사용됩니다. 덕분에 잘 정리된 수많은 자료들이 인터넷에 있어요. 이 글에서는 살짝 이론적인 관점에서 K-Means 클러스터링의 최적화 문제와 푸는 방법을 간단하게 살펴보고, 최종적으로는 Kernel Method를 활용해 K-Means 클러스터링의 표현력을 높이는 방법을 정리해보겠습니다.
1. K-Means 클러스터링 최적화 문제와 로이드 알고리즘 K-means 클러스터링은 서로 비슷한 위치에 있는 데이터들끼리 구분하여 클러스터로 묶어주는 비지도학습(Unsupervised Learning) 기법입니다. 구분되어있지 않은 데이터를 비슷한 종류끼리 묶고 싶거나, 데이터가 어떤 특징이 있는지 파악하고 싶을 때 사용할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Supervised Learning의 큰 그림</title>
      <link>http://repun.github.io/posts/supervised_learning/</link>
      <pubDate>Sat, 22 May 2021 16:37:57 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/supervised_learning/</guid>
      <description>이번에는 머신러닝에서 지도학습(Supervised Learning)의 전반적인 시나리오에 대한 글입니다. 지도학습으로 머신러닝 모델을 학습시킨다는 것이 무엇인지 설명하는 기초적인 이론이에요. 지도학습은 흔히 인풋 데이터 $x$가 주어졌을 때, 타겟 데이터 $y$를 추정하는 모델, 즉, $y=f(x)$가 되는 함수 $f$를 학습시키는 기법이라고 간단하게 설명됩니다. 하지만 우리는 이것보다 조금 더 근본적인 시각에서, 큰 그림으로 시작해볼게요. 확률 통계적인 관점에서 지도학습은 인풋 데이터 $x$가 주어졌을 때 타겟 $y$가 어떤 확률 분포를 따르는지, 조건부 확률 분포를 추정해 보는 것이라고 볼 수 있습니다.</description>
    </item>
    
  </channel>
</rss>
