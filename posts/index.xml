<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on MemoBlog</title>
    <link>http://repun.github.io/posts/</link>
    <description>Recent content in Posts on MemoBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 16 Oct 2022 20:47:24 +0900</lastBuildDate><atom:link href="http://repun.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CTC 디코딩을 위한 Prefix Beam Search</title>
      <link>http://repun.github.io/posts/prefix_beam_search/</link>
      <pubDate>Sun, 16 Oct 2022 20:47:24 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/prefix_beam_search/</guid>
      <description>이번 글은 Prefix Beam Search 알고리즘에 대해 정리하는 글입니다. Prefix Beam Search는 CTC(Connectionist Temporal Classification)를 이용해 학습된 모델의 디코딩에 사용됩니다. 평범한 Beam Search 알고리즘에서 CTC 디코딩에 맞게 살짝 변형시킨 알고리즘이 바로 Prefix Beam Search에요. 그러니 당연히 Prefix Beam Search에 대해 알기 위해서는 CTC와 Beam Search, 두 알고리즘에 대해 알고 있어야 합니다. 그래야 Prefix Beam Search 알고리즘이 왜 필요하고 어떻게 동작하는지 이해할 수 있습니다.
이 글은 Prefix Beam Search를 이해하는 데에 초점을 맞출 예정이에요.</description>
    </item>
    
    <item>
      <title>AMP(Automatic Mixed Precision)를 이용한 신경망 학습</title>
      <link>http://repun.github.io/posts/amp/</link>
      <pubDate>Mon, 08 Aug 2022 17:16:35 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/amp/</guid>
      <description>이번 글은 Automatic Mixed-Precision(AMP)를 이용해 신경망을 학습하는 원리를 간단하게 정리합니다. Mixed-Precision이라는건 말 그대로 Precision을 섞었다는 의미입니다. 보통 PyTorch나 TensorFlow 등의 딥러닝 프레임워크를 이용해 신경망 모델을 학습시킬 경우, 따로 설정을 하지 않는 이상 32-bit 크기의 소수 자료형을 이용해 계산합니다. 이 32-bit 크기의 소수 자료형을 Single Precision 혹은 FP32라고 부르지요. 하지만 32-bit의 절반, 16-bit만을 이용해 소수를 표현할 수도 있습니다. 이 자료형을 Half Precision 혹은 FP16 이라고 부릅니다. FP16은 FP32에 비해 메모리도 적게 먹고 빠르게 계산할 수 있지만, 표현할 수 있는 소수의 범위가 작습니다.</description>
    </item>
    
    <item>
      <title>딥러닝의 Attention Mechanism</title>
      <link>http://repun.github.io/posts/attention_mechanism/</link>
      <pubDate>Tue, 24 May 2022 17:17:44 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/attention_mechanism/</guid>
      <description>이번 글에서는 딥러닝에서 사용되는 어텐션(Attention) 메커니즘에 대해 정리하려고 합니다. 우선 어텐션이 어떤 논리로 작동하는지에 대해 간단하게 정리할게요. 기본이 되는 논리를 정확하게 이해하고 있으면 다양하게 제안되어온 수많은 어텐션의 구조와 수식을 이해하는데에 문제가 없습니다. 어텐션의 기본 논리 이후에는 어텐션이 사용된 예시를 몇가지 확인하도록 하겠습니다.
어텐션은 영단어의 뜻 그대로 집중이라는 의미입니다. 수많은 정보들 사이에서 내가 어떤 부분에 집중해야 하는가? 를 신경망으로 나타낸 구조라고 생각할 수 있어요. 신경망으로 집중을 나타낸다니 무슨 소리인지 감이 안올 수 있지만, 어텐션은 우리가 사전이나 데이터베이스에서 원하는 정보를 찾는 과정과 구조가 닮았습니다.</description>
    </item>
    
    <item>
      <title>Kernel K-Means 클러스터링의 원리</title>
      <link>http://repun.github.io/posts/kmeans_clustering/</link>
      <pubDate>Sun, 27 Feb 2022 09:52:08 +0900</pubDate>
      
      <guid>http://repun.github.io/posts/kmeans_clustering/</guid>
      <description>이번 글은 K-means 클러스터링 알고리즘에 대해 정리해보도록 하겠습니다. K-means 클러스터링은 간단하지만 성능이 좋아 자주 사용됩니다. 덕분에 잘 정리된 수많은 자료들이 인터넷에 있어요. 이 글에서는 살짝 이론적인 관점에서 K-Means 클러스터링의 최적화 문제와 푸는 방법을 간단하게 살펴보고, 최종적으로는 Kernel Method를 활용해 K-Means 클러스터링의 표현력을 높이는 방법을 정리해보겠습니다.
1. K-Means 클러스터링 최적화 문제와 로이드 알고리즘 K-means 클러스터링은 서로 비슷한 위치에 있는 데이터들끼리 구분하여 클러스터로 묶어주는 비지도학습(Unsupervised Learning) 기법입니다. 구분되어있지 않은 데이터를 비슷한 종류끼리 묶고 싶거나, 데이터가 어떤 특징이 있는지 파악하고 싶을 때 사용할 수 있습니다.</description>
    </item>
    
  </channel>
</rss>
